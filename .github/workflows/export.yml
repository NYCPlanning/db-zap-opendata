name: Daily Export

on:
  schedule:
    - cron: 0 0 * * *
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-20.04
    env:
      CLIENT_ID: ${{ secrets.CLIENT_ID }}
      SECRET: ${{ secrets.SECRET }}
      TENANT_ID: ${{ secrets.TENANT_ID }}
      ZAP_DOMAIN: ${{ secrets.ZAP_DOMAIN }}
      ZAP_ENGINE: ${{ secrets.ZAP_ENGINE }}
      AWS_S3_ENDPOINT: ${{ secrets.DO_S3_ENDPOINT }}
      AWS_ACCESS_KEY_ID: ${{ secrets.DO_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.DO_SECRET_ACCESS_KEY }}
      AWS_S3_BUCKET: edm-recipes
    strategy:
      fail-fast: false
      matrix:
        entity:
          - dcp_projects
          - dcp_projectactions
          - dcp_projectbbls
          - dcp_projectmilestones
          - dcp_projectactionbbls
          - dcp_communityboarddispositions
          - dcp_dcpprojectteams
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: '3.9' 
          
      - name: Install Dependencies
        run: python3 -m pip install .        
          
      - name: Get ${{ matrix.entity }}
        run: python3 -m src.runner ${{ matrix.entity }}
          
      - name: Set Version info
        id: version
        run: |
          DATE=$(date +%Y%m%d)
          echo "::set-output name=version::$DATE"

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@master
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID_DATA_ENGINEERING }}
          service_account_key: ${{ secrets.GCP_GCS_BQ_SA_KEY }}
          export_default_credentials: true
          
      - name: Archive to BigQuery
        run: |
          LOCATION=US
          VERSION=${{ steps.version.outputs.version }}
          NAME=${{ matrix.entity }}
          TABLENAME=$NAME.$VERSION
          FILEPATH=gs://zap-crm-export/datasets/$NAME/$VERSION/$NAME.csv
          
          # Upload to GCS
          gsutil cp .output/$NAME/$NAME.csv $FILEPATH

          # Import to BQ
          bq show $NAME || bq mk --location=$LOCATION --dataset $NAME
          bq show $TABLENAME || bq mk $TABLENAME
          bq load \
              --location=$LOCATION\
              --source_format=CSV\
              --quote '"' \
              --skip_leading_rows 1\
              --replace\
              --allow_quoted_newlines\
              $TABLENAME\
              $FILEPATH \ 
              schemas/$NAME.json
      
      - name: Archive to data library - PGdump
        env: 
          VERSION: ${{ steps.version.outputs.version }}
        run: |
          echo "exporting ${{ matrix.entity }}.sql"
          docker run --rm \
            -e AWS_S3_ENDPOINT=$AWS_S3_ENDPOINT\
            -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\
            -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\
            -e AWS_S3_BUCKET=$AWS_S3_BUCKET\
            -v $(pwd)/templates:/templates\
            -v $(pwd)/.output:/.output\
            nycplanning/library:ubuntu-01d9e5eddf65dbdea3eb0c500314963b5ec6246a library archive -f /templates/${{ matrix.entity }}.yml -v $VERSION -s -l -o pgdump --compress
          
      - name: Archive to data library - CSV
        env:
          VERSION: ${{ steps.version.outputs.version }}
        run: |
          echo "exporting ${{ matrix.entity }}.csv"
          docker run --rm \
            -e AWS_S3_ENDPOINT=$AWS_S3_ENDPOINT\
            -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\
            -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\
            -e AWS_S3_BUCKET=$AWS_S3_BUCKET\
            -v $(pwd)/templates:/templates\
            -v $(pwd)/.output:/.output\
            nycplanning/library:ubuntu-01d9e5eddf65dbdea3eb0c500314963b5ec6246a library archive -f /templates/${{ matrix.entity }}.yml -v $VERSION -s -l -o csv --compress
